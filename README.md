# **IberAutextification 2024**  :robot: 
# Introducci贸n 
La nueva generaci贸n de los grandes modelos de lenguaje (LLMs) es capaz de generar textos fluidos, coherentes y plausibles en muchos idiomas, volvi茅ndose dif铆cil de diferenciar del texto escrito por humanos. Esto ha generado la preocupaci贸n de que los LLMs puedan ser utilizados con fines maliciosos, como la generaci贸n de noticias falsas, la difusi贸n de opiniones polarizadas o el aumento de la credibilidad de campa帽as de phishing, entre otros. Estos ataques pueden realizarse en diferentes idiomas, dominios, modelos o estrategias de generaci贸n, lo que dificulta su moderaci贸n.

IberAuTexTification surge de la necesidad de desarrollar estrategias de moderaci贸n efectivas y generalizables para enfrentar estos modelos, cada vez m谩s sofisticados. Es la segunda versi贸n de la tarea compartida AuTextification en IberLEF 2023, ampliada a tres dimensiones: m谩s modelos, m谩s dominios y m谩s idiomas de la Pen铆nsula Ib茅rica.

En IberAuTexTification 2024 se introducen dos sub-tareas. La primera consiste en una clasificaci贸n binaria con dos clases: identificar si un texto fue escrito por un humano o generado por un LLM. La segunda sub-tarea es una clasificaci贸n multiclase en la que se busca identificar qu茅 modelo gener贸 el texto producido por un LLM. 

# Metodolog铆a  
El enfoque que nosotros adoptamos consiste en una arquitectura que incorpora Redes Neuronales de Grafos (GNN), Modelos Multiling眉es de Gran Escala (LLM) y caracter铆sticas estilom茅tricas. El diagrama general de la arquitectura de los modelos presentados en las subtareas compartidas se muestra a continuaci贸n 

![Descripci贸n de la imagen](https://drive.google.com/uc?export=view&id=1Zzm_o999lkIjJ1NZNQ_8NeghzvQORxaI)

Este repositorio presenta la arquitectura que emplea los LLMs junto con las caracter铆sticas estilom茅tricas, una de las tres arquitecturas propuestas en las subtareas. En este modelo, primero se realiza un fine-tuning a tres modelos de gran escala (BERT-Base-Multilingual, Multilingual-E5-Large, XLM-RoBERTa-Base). Luego, una vez que los LLMs han sido ajustados, se extraen los vectores embeddings de la 煤ltima capa de cada modelo. Esto se hace con el objetivo de capturar toda la informaci贸n contextual contenida en estos vectores y concatenarla con caracter铆sticas estilom茅tricas extra铆das directamente del corpus original.







## Datos
El conjunto de entrenamiento se encuentra [aqui](https://drive.google.com/drive/folders/1VdTmKAzrfFrL-MKEmsvEXjYKugrm5Rw7?usp=share_link)
Alli se puede almacenar cualquier cambio o procesamiento que se le haga al dataset

## Modelos 

En este repositorio se encuentran los experimentos realizados para la participaci贸n del equipo iimasNLP en las tareas compartidas de Identificaci贸n Automatizada de Textos en Lenguas de la Pen铆nsula Ib茅rica (IberAuTexTification 2024). 

La primer subtask consiste en determinar si un texto ha sido generado autom谩ticamente o no. La segunda subtask consiste en clasificar qu茅 modelo lo gener贸. Una novedad en esta edici贸n es detectar sobre un entorno multiling眉e, adem谩s de que se han a帽adido m谩s dominios y modelos. 

En cada carpeta, para la subtask_1 y subtask_2, se encuentra un notebook (Train_test_S1.ipynb/Train_test_S2.ipynb) que muestra la forma en la que se particionaron los datos de entrenamiento (70%) y prueba (30%), un script (LLM_S1.py/LLM_S2.py) con el que se generaron los vectores embeddings de los tres modelos 'Fine-Tuneados' que se utilizaron y, finalmente, un script (modelos_finales_S1.py/modelos_finales_S2.py) que contiene todas las posibles configuraciones utilizadas para los experimentos, incluyendo la configuraci贸n que se utiliz贸 para el modelo final.
